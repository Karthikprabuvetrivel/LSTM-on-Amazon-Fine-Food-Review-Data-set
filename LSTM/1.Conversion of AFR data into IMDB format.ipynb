{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of AFR data into IMDB format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    To convert the Amazon fine food dataset into IMDB datset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sqlite3\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is No NaN values in the DataFrame\n"
     ]
    }
   ],
   "source": [
    "#connecting database\n",
    "\n",
    "con=sqlite3.connect(\"database.sqlite\")\n",
    "\n",
    "# Read data from database\n",
    "\n",
    "raw_data=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3\"\"\",con)\n",
    "\n",
    "# Removal of Duplicates\n",
    "\n",
    "pre_data=raw_data.drop_duplicates(['UserId','ProfileName','Time','Text'],keep=\"first\")\n",
    "\n",
    "# Removal of Unconditioning data (denominator>numerator)\n",
    "\n",
    "pre_data=pre_data[pre_data.HelpfulnessNumerator<=pre_data.HelpfulnessDenominator]\n",
    "\n",
    "\n",
    "# Finding NaN values in dataframe\n",
    "\n",
    "# Reference\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html\n",
    "\n",
    "# Findind NaN values\n",
    "\n",
    "if pre_data.isnull().values.any() == False:\n",
    "    print(\"There is No NaN values in the DataFrame\")\n",
    "else:\n",
    "    print(\" There is NaN values present in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort data based on Time \n",
    "\n",
    "filter_data=pre_data.sort_values(by=[\"Time\"],axis=0)\n",
    "\n",
    "# Class Label changing\n",
    "# positive class label = 1\n",
    "# negative class label = 0\n",
    "a=[]\n",
    "for i in filter_data[\"Score\"]:\n",
    "    if i > 3:\n",
    "        a.append(1)\n",
    "    else:\n",
    "        a.append(0)\n",
    "filter_data[\"Score\"]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    307061\n",
       "0     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data[\"Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We took the Text column for the further review idendification task, because text is the most important feature compared to other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "# https://stackoverflow.com/a/40823105/4084039\n",
    "# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\n",
    "# https://stackoverflow.com/questions/18082130/python-regex-to-remove-all-words-which-contains-number/18082370#18082370\n",
    "# https://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string/5843547#5843547\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://gist.github.com/sebleier/554280\n",
    "# stemming tutorial: https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
    "# Lemmatisation tutorial: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "# NLTK Stemming package list: https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "stemmer=EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_data=filter_data[\"Text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "\n",
    "stopwords= set(['since','br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "# expanding contractions\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [07:12<00:00, 841.52it/s] \n"
     ]
    }
   ],
   "source": [
    "preprocessed_text_data=[]\n",
    "for i in tqdm(raw_text_data):\n",
    "# removing of HTML tags\n",
    "    a=re.sub(\"<.*?>\",\" \",i)\n",
    "# removing url\n",
    "    b=re.sub(r\"http\\S+\",\" \",a)\n",
    "# expanding contractions\n",
    "    c=decontracted(b)\n",
    "# removing alpha_numeric\n",
    "    d=re.sub(\"\\S*\\d\\S*\", \" \",c)\n",
    "# removing Special characters\n",
    "    e=re.sub('[^A-Za-z0-9]+', ' ',d)\n",
    "# removing stopwords\n",
    "    k=[]\n",
    "    for w in e.split():\n",
    "        if w.lower() not in stopwords:\n",
    "            s=(stemmer.stem(w.lower())).encode('utf8')\n",
    "            k.append(s)\n",
    "    preprocessed_text_data.append(b' '.join(k).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_data[\"Text\"]=preprocessed_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conversion of Data into IMDB format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://github.com/PushpendraSinghChauhan/Amazon-Fine-Food-Reviews/blob/master\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=filter_data.Text\n",
    "y=filter_data.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291336,)\n",
      "(72835,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Getting vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=CountVectorizer()\n",
    "bow.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting vocabulary\n",
    "\n",
    "vocabulary=bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64868"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291336/291336 [00:11<00:00, 26316.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting word frequency and indexing\n",
    "\n",
    "word_list=dict()\n",
    "index=0\n",
    "for i in tqdm(x_train.values):\n",
    "    for j in i.split():\n",
    "        word_list.setdefault(j,[])\n",
    "        word_list[j].append(index)\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64887"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64868/64868 [00:00<00:00, 469389.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting word frequency length\n",
    "\n",
    "word_freq=[]\n",
    "for i in tqdm(vocabulary):\n",
    "    word_freq.append(len(word_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq=np.asarray(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort by Frequency\n",
    "\n",
    "freq_index=np.argsort(word_freq)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38986, 32656, 56247, ..., 35021, 35017, 32433])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rank as per the frequency \n",
    "\n",
    "freq_rank=dict()\n",
    "rank=1\n",
    "for i in freq_index:\n",
    "    freq_rank[vocabulary[i]] = rank\n",
    "    rank +=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291336/291336 [00:09<00:00, 29802.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# each word into rank conversion (train_data)\n",
    "\n",
    "X_train=[]\n",
    "for i in tqdm(x_train.values):\n",
    "    row_data=[]\n",
    "    for j in i.split():\n",
    "        if (len(j)>1):\n",
    "            row_data.append(freq_rank[j])\n",
    "    X_train.append(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72835/72835 [00:02<00:00, 35703.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# test data \n",
    "\n",
    "X_test=[]\n",
    "for i in tqdm(x_test.values):\n",
    "    row_data=[]\n",
    "    for j in i.split():\n",
    "        try:\n",
    "            if (len(j)>1):\n",
    "                row_data.append(freq_rank[j])\n",
    "        except KeyError:\n",
    "            row_data.append(0)\n",
    "    X_test.append(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291336\n",
      "72835\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# length of the document 1\n",
    "\n",
    "print(len(X_train[0]))\n",
    "print(len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word review\n",
      "====================================================================================================\n",
      "tri steer famili organ side food chain resist complain often not give often mac n chees anni sat shelf ignor donat number version took chanc order case price good amazon case got eaten month fall back dinner hubbi make kid not around yesterday husband pull left kraft box made regret told not hold candl stuff go figur think speechless minut kraft torch noth els compar flavor came back buy sold\n",
      "Corresponding Rank of review\n",
      "====================================================================================================\n",
      "[11, 3643, 187, 102, 341, 17, 1762, 1903, 1081, 419, 1, 55, 419, 1222, 225, 1770, 2082, 792, 2096, 2447, 746, 369, 352, 858, 23, 209, 26, 5, 24, 209, 90, 580, 127, 849, 114, 557, 1978, 16, 186, 1, 196, 1646, 259, 975, 452, 1367, 41, 59, 1460, 638, 1, 603, 4355, 96, 38, 624, 60, 11423, 237, 1367, 11973, 265, 345, 273, 4, 207, 114, 19, 562]\n",
      "word review\n",
      "====================================================================================================\n",
      "littl sweet side not tast like strong vodka usual enjoy dessert drink still good strong vodka sweet flavor kill alcohol tast result smooth drink usual mix littl bit ice water perfect\n",
      "Corresponding Rank of review\n",
      "====================================================================================================\n",
      "[29, 46, 341, 1, 3, 2, 151, 2216, 178, 66, 833, 33, 84, 5, 151, 2216, 46, 4, 1328, 1263, 3, 411, 284, 33, 178, 42, 29, 78, 204, 45, 86]\n"
     ]
    }
   ],
   "source": [
    "# Train document values\n",
    "print(\"word review\")\n",
    "print(\"=\"*100)\n",
    "print(x_train.values[0])\n",
    "\n",
    "print(\"Corresponding Rank of review\")\n",
    "print(\"=\"*100)\n",
    "print(X_train[0])\n",
    "\n",
    "# Test\n",
    "\n",
    "print(\"word review\")\n",
    "print(\"=\"*100)\n",
    "print(x_test.values[0])\n",
    "\n",
    "print(\"Corresponding Rank of review\")\n",
    "print(\"=\"*100)\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of the file \n",
    "\n",
    "type(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291336"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72835"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    245654\n",
       "0     45682\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    61407\n",
       "0    11428\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Exporting of data in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.programiz.com/python-programming/working-csv-files\n",
    "# https://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "# https://www.programcreek.com/python/example/99451/sklearn.externals.joblib.dump\n",
    "\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_train.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "\n",
    "joblib.dump(X_train,\"x_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_test.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "\n",
    "joblib.dump(X_test,\"x_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_train.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train label\n",
    "\n",
    "joblib.dump(y_train,\"y_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test label\n",
    "\n",
    "joblib.dump(y_test,\"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Text preprocessing of Amazon fine food review(AFR) dataset was completed by using typical methods like stemming, stop words,tag removal using regular expression and many other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: The AFR dataset was converted into IMDB dataset format by using frequency of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: After the conversion of imdb dataset format,the data's stored in the format of pickle for the further classification process using LSTM at google colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
